<!DOCTYPE html>
    <head>

        
        

    </head>

    <body> 

        
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-164308157-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

        <div class="header" id="page_menu" >
  <nav class="navbar sticky-top navbar-expand-lg navbar-dark bg-dark">
    <a class="navbar-brand" href="/"> <img src="/banner/merrysprout-logo.png" style="width:50px;" />merry sprout</a>
    <span class="navbar-text">
      Inspiring roboticist
    </span>

    
    <div class="navbar-nav ml-auto">
      <a class="navbar-text " href="/about/about">About</a>
    </div>
  </nav>    
</div> <br>

        <div class="container" id="frame">

            

<div class="row blog-content">
    <div class="col-md-12 col-sm-6">
        <h2>Brief Survey of Position Estimation Systems</h2><small class="text-muted">9 mins</small>
    </div>
</div>

 
<div class="row">
    <div class="col-md-12 col-sm-6">
        <img src="/banner/position_estimator_survey.jpg" class="img-fluid" alt="position_estimator_survey.jpg"> <hr>        
    </div>
</div>

<div class="row blog-content">
    <div class="col-md-12 col-sm-6">
        <p>I am starting a new project and I was wondering what would be the appropriate position estimation system to use. I have used a couple over the years. Each of them have their own strengths and weaknesses. I thought it would be a good exercise to go over them briefly to show how they differ and which one to use.</p>
<h2 id="classification-based-on-operation-mode">Classification based on operation mode</h2>
<p>One way to classify these systems is based on the way they operate on the data. There are two broad categories:</p>
<h3 id="filtering-based">Filtering based</h3>
<p>These types of systems use some variant of kalman filter like the vanilla, extended or unscented kalman filter. When a new sensor measurement comes in, they integrate into the current state based on the confidence (covariance) measurements. Filtering based methods are relatively fast as they deal with implementing a few simple equations. However since they donâ€™t store the previous states, it is generally not possible to go back and alter the previous states if we find out that we have drifted (when we identify a loop closure). There are some implementations that store some of the previous states or all the previous states and update them when loop closure happens, but these are generally computationally intensive and not real-time.</p>
<h3 id="smoothing-based">Smoothing based</h3>
<p>These types of systems use some kind of graph based methods to integrate their measurements. They store all the states and solve for all of them jointly at every step. One major advantage of graph based methods is that they have a way to update the previous measurements in the case of loop closures without much hassle. They are intrinsically good at that. However as the number of states increases, it becomes computationally intractable. So they were not very popular for estimating positions of robots. The ground breaking work of <code>Incremental Smoothing and Mapping</code> (iSAM[1] &amp; iSAM2[2]) by  <a href="http://www.cs.cmu.edu/~kaess/">Michael Kaess</a> changed this. They figured out a method to keep the computations tractable even for very large problems. It is a clever method to include entries into the graph and maintain sparsity which are much more efficient to operate on compared to dense methods. With more computing at our disposal and efficient ways of operating the graph, more and more systems are opting to use graph based methods for estimating position.</p>
<h2 id="classification-based-on-sensors-used">Classification based on sensors used</h2>
<p>Another way to classify these systems is based on the primary sensors they use. There are three major types that are popular these days:
<figure><img src="../position_estimators_overview.png"
         alt="Classification of position estimators based on sensors used" width="100%"/><figcaption>
            <p>Classification of position estimators based on sensors used</p>
        </figcaption>
</figure>
</p>
<h2 id="lidar-based">LIDAR based</h2>
<p>With the advent of self driving cars, the use of LIDAR based systems have skyrocketed. LIDARs are relatively new to the field compared to cameras. Particularly they became famous after the DARPA Grand Challenge in 2006. Velodyne has been the market leader since then, making these LIDAR systems. However there are a lot of new entrants who are giving a tough competition to Velodyne. I heard that Velodyne was one of the competitors in the Grand Challenge. Their autonomy performed poorly but the sensor they developed worked really well and was an envy for many teams. So, after the competition they rolled that out as a product and are very successful with that.</p>
<h3 id="iterative-closest-point-icp">Iterative Closest Point (ICP)</h3>
<p><a href="https://en.wikipedia.org/wiki/Iterative_closest_point">ICP</a> is an iterative algorithm which tries to align/stitch point clouds. It was published in the 90&rsquo;s. They became pretty famous with the LIDAR based sensors. The ICP algorithm tries to minimize the distance between the associated points. Multiple variants like point-to-plane and plane-to-plane algorithms exist as well. These methods are generally computationally intensive.  But the accuracy is much higher compared to other methods like using cameras. However the LIDAR sensors are costly as well. With the recent competition the price is dropping substantially.</p>
<h3 id="lidar-odometry-and-mapping-loam">Lidar Odometry and Mapping (LOAM)</h3>
<p>LOAM[3] is the state-of-the-art LIDAR matching agorithm by <a href="https://frc.ri.cmu.edu/~zhangji/">Ji Zhang</a>. LOAM beats all the other state estimation algorithms in accuracy and has stayed on top of the pack since its publication. Instead of using ICP on the whole point cloud, they extract corner and edge features from the point cloud and use those for matching. These features help to summarize the information and generalize well to varied environments.</p>
<h2 id="camera-based">Camera based</h2>
<p>If LIDAR is not an option, cameras are very good alternatives. Cameras and the associated computing has become so cheap, they could be used in any project. Camera based system can either use single camera (monocular), two cameras (binocular/stereo) or more. Depending on the density of the map, there are three different ways in which the camera system can be setup.</p>
<ul>
<li><strong>Sparse methods</strong><br>
These are suited for situations where we care about the position estimate and not about the actual map (ie., localization and not mapping in SLAM).</li>
<li><strong>Dense methods</strong><br>
For situations where we need a dense map which is consistent then we opt for the dense methods. However these methods are computationally expensive.</li>
<li><strong>Semi-dense methods</strong><br>
These are methods which use the dense method principles but maintain some level of sparsity for keeping the computation cost lower.</li>
</ul>
<h3 id="sparse-methods">Sparse methods</h3>
<p>The general operating procedure is as follows:</p>
<ul>
<li>Detect features in the image using feature descriptors like SIFT/SURF or ORB.</li>
<li>Associate the features to the same features in a different image (from previous images in the monocular case and other camera in the binocular case)</li>
<li>Calculate the depth (or 3d position of the feature observed) using some form of triangulation</li>
<li>Estimate the change in position of the robot based on how these features move in the scene</li>
<li>Use the estimated position to accumate the points to build a sparse map (generally done using a separate thread)</li>
<li>Continue tracking these features as long as possible</li>
</ul>
<h4 id="parallel-tracking-and-mapping-ptam">Parallel Tracking and Mapping (PTAM)</h4>
<p>PTAM[4] is a landmark work where they seperated the tracking and mapping processes into separate threads. This allowed tracking in real-time. Meanwhile the mapping process which is computationally heavy was not tied to real-time performance and hence could run longer and produce better results. The better results from the mapping process was then fed back to correct tracking. Such a separation is the main insight that came from the PTAM work which was pretty new at that time.</p>
<h4 id="orb-slam-and-orb-slam2">ORB-SLAM and ORB-SLAM2</h4>
<p>ORB-SLAM[5] is another landmark paper which showcased a robustified SLAM solution which in addition to tracking and mapping also showed loop-closure using the bag-of-words technique. Though none of these techniques were completely new, ORB-SLAM put them together in a way that made it fast, versatile and robust. They are so good, you can download the code and throw in some camera data and get them working in minimal time. One of the major limitations of PTAM was that it did not scale well with exploration. ORB-SLAM solved this problem in a elegant way using covisibility graphs. The ORB-SLAM2 extended the previous work to stereo camera setup.</p>
<h4 id="visual-lidar-odometry-and-mapping-vloam">Visual Lidar Odometry and Mapping (VLOAM)</h4>
<p>VLOAM[6] is an extension of the LOAM work but it employes a monocular camera as well. This work introduced a formulation where visual features, both with and without depth can be used to estimate the motion. This allowed some long distance visual features which are not observed by the lidar to be used which provided stability in bearing. It also gets the added benefit of LOAM. To date (2020) this is the best method on the KITTI dataset.</p>
<h3 id="dense-methods">Dense methods</h3>
<p>Dense methods take a different approach to position estimation. Instead of tracking individual feature points, they employ the whole image and calculate photometric error for every pixel in the image. This makes the process computationally heavy but leads to a very dense map/pointcloud.</p>
<h4 id="kinectfusion">KinectFusion</h4>
<p>KinectFusion[7] is one of the earlier dense methods which worked in real-time using a RGB-D sensor. This work leverages the insights from PTAM by separating the mapping and tracking processes. This helps it to maintain a very high tracking frame-rate which is crucial for the &lsquo;small motion&rsquo; assumption and also helps the mapping process which uses a modified ICP algorithm.</p>
<h4 id="dense-tracking-and-mapping-in-real-time-dtam">Dense tracking and Mapping in Real-time (DTAM)</h4>
<p>DTAM[8] is from Richard A. Newcombe who originally developed KinectFusion. It operates on monocular cameras! (instead of RGB-D camera). They estimate depth on keyframes and perform KinectFusion on those keyframes. The key insight is the prediction of depth for each pixel from a range of candidate depths which is precomputed.</p>
<h3 id="semi-dense-methods">Semi-dense methods</h3>
<p>As opposed to using keypoints, semi-dense methods reduce photometric error similar to dense methods to create semi-dense depth maps which are then fused using dense method techniques.</p>
<h4 id="large-scale-direct-monocular-slam-lsd-slam">Large Scale Direct Monocular SLAM (LSD-SLAM)</h4>
<p>LSD-SLAM[9] is one of the famous semi-dense methods which demonstrated tracking and mapping on a large-scale dataset using photometric formulation. Dense methods typically fall short when it comes to mapping larger areas because they run into memory and computation constraints. LSD-SLAM tackles this issue by constructing semi-dense depth maps.</p>
<h2 id="camera-and-imu-based">Camera and IMU based</h2>
<p>Data from the IMU is generally very noisy. In addition to the noise, the sensor data has biases which are hard to estimate as the biases keep drifting. However recent advancements in the IMU technology has led to sensors which are much cheaper and less noisy. This has led to software systems which can better estimate the biases and form a tightly coupled system commonly referred to as visual-inertial systems. Of course the LIDAR based systems and camera based systems use IMU data. But they are just used as initial estimates in the optimization process. However in visual inertial systems they are tightly integrated.</p>
<h3 id="vins-mono--vins-fusion">VINS Mono &amp; VINS Fusion</h3>
<p>VINS-Mon[10] is a monocular odometry system which tightly incorporates the IMU (visual-inertial). VINS-Mono incorporates IMU preintegration which propogates the bias estimates to previous steps and update the IMU measurements without have to recompute everything. This is a smart formulation which leads to efficient computations and better position estimates. VINS-Fusion is an extension of VINS-Mono which supports stereo cameras.</p>
<h3 id="visual-inertial-lidar-slam-vil-slam">Visual Inertial Lidar SLAM (VIL SLAM)</h3>
<p>VIL-SLAM[11] builds and VINS Mono and VLOAM. It implements a visual-inertial system for odometry which is then fed to lidar mapping. It uses visual-bag-of-words for loop closure and refines it further using ICP.</p>
<h2 id="references">References</h2>
<p>[1] Michael Kaess &ldquo;<a href="http://www.cs.cmu.edu/~kaess/pub/Kaess08thesis.pdf">Incremental Smoothing and Mapping</a>&rdquo; Ph.D dissertation, Dec 2008<br>
[2] Michael Kaess, et al., &ldquo;<a href="http://www.cs.cmu.edu/~kaess/pub/Kaess12ijrr.pdf">iSAM2: Incremental Smoothing and Mapping Using the Bayes Tree</a>&rdquo;, Intl. J. of Robotics Research, Feb 2012<br>
[3] Ji Zhang and Sanjiv Singh &ldquo;<a href="https://ri.cmu.edu/pub_files/2014/7/Ji_LidarMapping_RSS2014_v8.pdf">LOAM: Lidar Odometry and Mapping in Real-time</a>&rdquo;, RSS, 2014<br>
[4] Georg Klein and David Murray &ldquo;<a href="http://www.robots.ox.ac.uk/~gk/publications/KleinMurray2007ISMAR.pdf">Parallel Tracking and Mapping for Small AR Workspaces</a>&rdquo;, International Symposium on Mixed and Augmented Reality, 2007<br>
[5] Raul Mur-Artal, J. M. M. Montiel, Juan D. Tardos &ldquo;<a href="https://arxiv.org/abs/1502.00956">ORB-SLAM: a Versatile and Accurate Monocular SLAM System</a>&rdquo;, IEEE transactions on robotics, 2015<br>
[6] Ji Zhang and Sanjiv Singh &ldquo;<a href="https://frc.ri.cmu.edu/~zhangji/publications/ICRA_2015.pdf">Visual-lidar Odometry and Mapping: Low-drift, Robust and Fast</a>&rdquo; IEEE International Conference on Robotics and Automation (ICRA), 2015<br>
[7] Richard A. Newcombe et al., &ldquo;<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf">KinectFusion: Real-Time Dense Surface mapping and Tracking</a>&rdquo; IEEE International Symposium on Mixed and Augmented Reality, 2011<br>
[8] Richard A. Newcombe et al., &ldquo;DTAM: Dense tracking and mapping in real-time&rdquo; ICCV 2011<br>
[9] Jakob Engel, et al., &ldquo;<a href="https://vision.in.tum.de/_media/spezial/bib/engel14eccv.pdf">LSD-SLAM: Large-Scale Direct Monocular SLAM</a>&rdquo; European Conference on Computer Vision 2014<br>
[10] Tong Qin, Peiliang Li and Shaojie Shen &ldquo;<a href="https://arxiv.org/pdf/1708.03852.pdf">VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator</a>&rdquo; IEEE Transactions on Robotics 2017
[11] Weizhao Shao, et al., &ldquo;<a href="https://arxiv.org/abs/1902.10741">Stereo Visual Inertial LiDAR Simultaneous Localization and Mapping</a>&rdquo; IROS 2019</p>

    </div>
</div>
<br>
<div class="row">
    <div class="col-md-12 col-sm-6">
        


<div id="sharing">
  <a href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fwww.merrysprout.com%2fposts%2fstate_estimation%2fposition_estimator_survey%2f" class="facebook" aria-label="share on Facebook">
    <svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="126.445 2.281 589 589"><circle cx="420.945" cy="296.781" r="294.5" fill="#3c5a9a"/><path d="M516.704 92.677h-65.239c-38.715 0-81.777 16.283-81.777 72.402.189 19.554 0 38.281 0 59.357H324.9v71.271h46.174v205.177h84.847V294.353h56.002l5.067-70.117h-62.531s.14-31.191 0-40.249c0-22.177 23.076-20.907 24.464-20.907 10.981 0 32.332.032 37.813 0V92.677h-.032z" fill="#fff"/></svg>
  </a>

  <a href="http://twitter.com/share?url=https%3a%2f%2fwww.merrysprout.com%2fposts%2fstate_estimation%2fposition_estimator_survey%2f&text=Brief%20Survey%20of%20Position%20Estimation%20Systems&via=" class="twitter" aria-label="share on Twitter">
    <svg xmlns="http://www.w3.org/2000/svg" width="50" height="40" viewBox="-0.25 -0.25 1109.5 901.5"><path d="M741 .2V0h52l19 3.8c12.667 2.467 24.167 5.7 34.5 9.7 10.334 4 20.334 8.667 30 14 9.667 5.333 18.434 10.767 26.301 16.3 7.8 5.467 14.8 11.267 21 17.4C929.933 67.4 939.5 69 952.5 66s27-7.167 42-12.5 29.834-11.333 44.5-18c14.667-6.667 23.601-10.9 26.801-12.7 3.133-1.866 4.8-2.866 5-3l.199-.3 1-.5 1-.5 1-.5 1-.5.2-.3.3-.2.301-.2.199-.3 1-.3 1-.2-.199 1.5-.301 1.5-.5 1.5-.5 1.5-.5 1-.5 1-.5 1.5c-.333 1-.666 2.333-1 4-.333 1.667-3.5 8.333-9.5 20S1051 73 1042 85s-17.066 21.066-24.199 27.2c-7.2 6.2-11.967 10.533-14.301 13-2.333 2.533-5.166 4.866-8.5 7l-5 3.3-1 .5-1 .5-.199.3-.301.2-.3.2-.2.3-1 .5-1 .5-.199.3-.301.2-.3.2-.2.3-.199.3-.301.2-.3.2-.2.3h5l28-6c18.667-4 36.5-8.833 53.5-14.5l27-9 3-1 1.5-.5 1-.5 1-.5 1-.5 1-.5 2-.3 2-.2v2l-.5.2-.5.3-.199.3-.301.2-.3.2-.2.3-.199.3-.301.2-.3.2-.2.3-.199.3-.301.2-.5 1-.5 1-.3.2c-.133.2-4.366 5.866-12.7 17-8.333 11.2-12.833 16.866-13.5 17-.666.2-1.6 1.2-2.8 3-1.133 1.866-8.2 9.3-21.2 22.3s-25.732 24.566-38.199 34.7c-12.533 10.2-18.867 22.733-19 37.6-.2 14.8-.967 31.534-2.301 50.2-1.333 18.667-3.833 38.833-7.5 60.5-3.666 21.667-9.333 46.167-17 73.5-7.666 27.333-17 54-28 80s-22.5 49.333-34.5 70-23 38.167-33 52.5-20.166 27.833-30.5 40.5c-10.333 12.667-23.399 26.934-39.199 42.8-15.867 15.8-24.533 24.467-26 26-1.533 1.467-8.066 6.934-19.601 16.4-11.466 9.533-23.8 19.066-37 28.6-13.133 9.467-25.2 17.367-36.2 23.7s-24.266 13.566-39.8 21.7C630.734 840.4 614 848 596 855s-37 13.5-57 19.5-39.333 10.667-58 14c-18.666 3.333-39.833 6.167-63.5 8.5l-35.5 3.5v.5h-65v-.5l-8.5-.5c-5.666-.333-10.333-.667-14-1-3.666-.333-17.5-2.167-41.5-5.5s-42.833-6.667-56.5-10c-13.666-3.333-34-9.667-61-19s-50.1-18.767-69.3-28.3c-19.133-9.467-31.133-15.467-36-18-4.8-2.467-10.2-5.533-16.2-9.2l-9-5.5-.199-.3-.301-.2-.3-.2-.2-.3-1-.5-1-.5-.199-.3-.301-.2-.3-.2-.2-.3-.199-.3L.5 800H0v-2l1 .2 1 .3 4.5.5c3 .333 11.167.833 24.5 1.5 13.334.667 27.5.667 42.5 0s30.334-2.167 46-4.5c15.667-2.333 34.167-6.333 55.5-12 21.334-5.667 40.934-12.4 58.801-20.2 17.8-7.866 30.466-13.733 38-17.6 7.466-3.8 18.866-10.867 34.199-21.2l23-15.5.2-.3.3-.2.301-.2.199-.3.2-.3.3-.2.301-.2.199-.3 1-.3 1-.2.2-1 .3-1 .301-.2.199-.3-8-.5c-5.333-.333-10.5-.667-15.5-1s-12.833-1.833-23.5-4.5c-10.666-2.667-22.166-6.667-34.5-12-12.333-5.333-24.333-11.667-36-19-11.666-7.333-20.1-13.434-25.3-18.3-5.133-4.801-11.8-11.6-20-20.4-8.133-8.866-15.2-17.967-21.2-27.3s-11.733-20.101-17.199-32.3L124.5 551l-.5-1.5-.5-1.5-.3-1-.2-1 1.5.2 1.5.3 11 1.5c7.334 1 18.834 1.333 34.5 1 15.667-.333 26.5-1 32.5-2s9.667-1.667 11-2l2-.5 2.5-.5 2.5-.5.2-.3.3-.2.301-.2.199-.3-2-.5-2-.5-2-.5-2-.5-2-.5c-1.333-.333-3.666-1-7-2-3.333-1-12.333-4.667-27-11-14.666-6.333-26.333-12.5-35-18.5a241.7 241.7 0 0 1-24.8-19.7c-7.8-7.2-16.366-16.467-25.7-27.8-9.333-11.333-17.666-24.5-25-39.5-7.333-15-12.833-29.333-16.5-43a232.143 232.143 0 0 1-7.199-41.5L43 316l1 .2 1 .3 1 .5 1 .5 1 .5 1 .5 15.5 7c10.334 4.667 23.167 8.667 38.5 12 15.334 3.333 24.5 5.167 27.5 5.5l4.5.5h9l-.199-.3-.301-.2-.3-.2-.2-.3-.199-.3-.301-.2-.3-.2-.2-.3-1-.5-1-.5-.199-.3-.301-.2-.3-.2-.2-.3-1-.5-1-.5-.199-.3c-.2-.134-3.067-2.267-8.601-6.4-5.467-4.2-11.2-9.633-17.2-16.3s-12-13.667-18-21A162.158 162.158 0 0 1 77 271c-4.666-8.333-9.6-18.934-14.8-31.8-5.133-12.8-9.033-25.7-11.7-38.7-2.666-13-4.166-25.833-4.5-38.5-.333-12.667 0-23.5 1-32.5s3-19.167 6-30.5 7.334-23.333 13-36l8.5-19 .5-1.5.5-1.5.301-.2.199-.3.2-.3.3-.2.301.2.199.3.2.3.3.2.301.2.199.3.2.3.3.2.5 1 .5 1 .301.2.199.3 13.5 15c9 10 19.667 21.167 32 33.5 12.334 12.333 19.167 18.733 20.5 19.2 1.334.533 3 2.066 5 4.6 2 2.467 8.667 8.367 20 17.7 11.334 9.333 26.167 20.167 44.5 32.5 18.334 12.333 38.667 24.5 61 36.5 22.334 12 46.334 22.833 72 32.5 25.667 9.667 43.667 16 54 19 10.334 3 28 6.833 53 11.5s43.834 7.667 56.5 9c12.667 1.333 21.334 2.1 26 2.3l7 .2-.199-1.5-.301-1.5-2-12.5c-1.333-8.333-2-20-2-35s1.167-28.833 3.5-41.5c2.334-12.667 5.834-25.5 10.5-38.5 4.667-13 9.234-23.434 13.7-31.3 4.534-7.8 10.467-16.7 17.8-26.7 7.334-10 16.834-20.333 28.5-31 11.667-10.667 25-20.167 40-28.5s28.834-14.667 41.5-19c12.667-4.333 23.334-7.167 32-8.5 8.667-1.333 13-2.1 13-2.3z" fill="#5da8dc" stroke="#5da8dc" stroke-width=".5"/><path d="M0 399V0h741v.2c0 .2-4.333.966-13 2.3-8.666 1.333-19.333 4.167-32 8.5-12.666 4.333-26.5 10.667-41.5 19s-28.333 17.833-40 28.5c-11.666 10.667-21.166 21-28.5 31-7.333 10-13.266 18.9-17.8 26.7-4.466 7.866-9.033 18.3-13.7 31.3-4.666 13-8.166 25.833-10.5 38.5-2.333 12.667-3.5 26.5-3.5 41.5s.667 26.667 2 35l2 12.5.301 1.5.199 1.5-7-.2c-4.666-.2-13.333-.966-26-2.3-12.666-1.333-31.5-4.333-56.5-9s-42.666-8.5-53-11.5c-10.333-3-28.333-9.333-54-19-25.666-9.667-49.666-20.5-72-32.5-22.333-12-42.666-24.167-61-36.5-18.333-12.333-33.166-23.167-44.5-32.5-11.333-9.333-18-15.233-20-17.7-2-2.533-3.666-4.066-5-4.6-1.333-.467-8.166-6.867-20.5-19.2-12.333-12.333-23-23.5-32-33.5L80 44.5l-.199-.3-.301-.2-.5-1-.5-1-.3-.2-.2-.3-.199-.3-.301-.2-.3-.2-.2-.3-.199-.3-.301-.2-.3.2-.2.3-.199.3-.301.2-.5 1.5-.5 1.5L66 63c-5.666 12.667-10 24.667-13 36s-5 21.5-6 30.5-1.333 19.833-1 32.5c.334 12.667 1.834 25.5 4.5 38.5 2.667 13 6.567 25.9 11.7 38.7 5.2 12.866 10.134 23.466 14.8 31.8 4.667 8.333 10 16.167 16 23.5 6 7.333 12 14.333 18 21s11.733 12.1 17.2 16.3c5.533 4.134 8.4 6.267 8.601 6.4l.199.3 1 .5 1 .5.2.3.3.2.301.2.199.3 1 .5 1 .5.2.3.3.2.301.2.199.3.2.3.3.2.301.2.199.3h-9l-4.5-.5c-3-.333-12.166-2.167-27.5-5.5-15.333-3.333-28.166-7.333-38.5-12l-15.5-7-1-.5-1-.5-1-.5-1-.5-1-.3-1-.2 1.801 21c1.133 14 3.533 27.833 7.199 41.5 3.667 13.667 9.167 28 16.5 43 7.334 15 15.667 28.167 25 39.5 9.334 11.333 17.9 20.6 25.7 27.8a241.7 241.7 0 0 0 24.8 19.7c8.667 6 20.334 12.167 35 18.5 14.667 6.333 23.667 10 27 11 3.334 1 5.667 1.667 7 2l2 .5 2 .5 2 .5 2 .5 2 .5-.199.3-.301.2-.3.2-.2.3-2.5.5-2.5.5-2 .5c-1.333.333-5 1-11 2s-16.833 1.667-32.5 2c-15.666.333-27.166 0-34.5-1l-11-1.5-1.5-.3-1.5-.2.2 1 .3 1 .5 1.5.5 1.5 8.301 18.2C138.266 581.399 144 592.167 150 601.5s13.067 18.434 21.2 27.3c8.2 8.801 14.867 15.6 20 20.4 5.2 4.866 13.634 10.967 25.3 18.3 11.667 7.333 23.667 13.667 36 19 12.334 5.333 23.834 9.333 34.5 12 10.667 2.667 18.5 4.167 23.5 4.5s10.167.667 15.5 1l8 .5-.199.3-.301.2-.3 1-.2 1-1 .2-1 .3-.199.3-.301.2-.3.2-.2.3-.199.3-.301.2-.3.2-.2.3-23 15.5c-15.333 10.333-26.733 17.4-34.199 21.2-7.534 3.866-20.2 9.733-38 17.6-17.867 7.8-37.467 14.533-58.801 20.2-21.333 5.667-39.833 9.667-55.5 12-15.666 2.333-31 3.833-46 4.5s-29.166.667-42.5 0c-13.333-.667-21.5-1.167-24.5-1.5l-4.5-.5-1-.3-1-.2V399zM1107.801 109.8l.199-.3.5-.3.5-.2v792H382v-.5l35.5-3.5c23.667-2.333 44.834-5.167 63.5-8.5 18.667-3.333 38-8 58-14s39-12.5 57-19.5 34.734-14.6 50.2-22.8c15.534-8.134 28.8-15.367 39.8-21.7s23.067-14.233 36.2-23.7c13.2-9.533 25.534-19.066 37-28.6 11.534-9.467 18.067-14.934 19.601-16.4 1.467-1.533 10.133-10.2 26-26 15.8-15.866 28.866-30.133 39.199-42.8 10.334-12.667 20.5-26.167 30.5-40.5s21-31.833 33-52.5 23.5-44 34.5-70 20.334-52.667 28-80c7.667-27.333 13.334-51.833 17-73.5 3.667-21.667 6.167-41.833 7.5-60.5 1.334-18.667 2.101-35.4 2.301-50.2.133-14.866 6.467-27.4 19-37.6 12.467-10.134 25.199-21.7 38.199-34.7s20.067-20.434 21.2-22.3c1.2-1.8 2.134-2.8 2.8-3 .667-.134 5.167-5.8 13.5-17 8.334-11.134 12.567-16.8 12.7-17l.3-.2.5-1 .5-1 .301-.2.199-.3.2-.3.3-.2.301-.2.199-.3.2-.3.3-.2.301-.2zM812 3.8L793 0h316v107l-2 .2-2 .3-1 .5-1 .5-1 .5-1 .5-1.5.5-3 1-27 9c-17 5.667-34.833 10.5-53.5 14.5l-28 6h-5l.2-.3.3-.2.301-.2.199-.3.2-.3.3-.2.301-.2.199-.3 1-.5 1-.5.2-.3.3-.2.301-.2.199-.3 1-.5 1-.5 5-3.3c3.334-2.134 6.167-4.467 8.5-7 2.334-2.467 7.101-6.8 14.301-13C1024.933 106.066 1033 97 1042 85s16.5-23.833 22.5-35.5 9.167-18.333 9.5-20c.334-1.667.667-3 1-4l.5-1.5.5-1 .5-1 .5-1.5.5-1.5.301-1.5.199-1.5-1 .2-1 .3-.199.3-.301.2-.3.2-.2.3-1 .5-1 .5-1 .5-1 .5-.199.3c-.2.134-1.867 1.134-5 3-3.2 1.8-12.134 6.034-26.801 12.7-14.666 6.667-29.5 12.667-44.5 18s-29 9.5-42 12.5-22.566 1.4-28.699-4.8c-6.2-6.134-13.2-11.934-21-17.4-7.867-5.533-16.634-10.966-26.301-16.3a245.399 245.399 0 0 0-30-14c-10.333-4-21.833-7.233-34.5-9.7zM0 850.5V800h.5l.301.2.199.3.2.3.3.2.301.2.199.3 1 .5 1 .5.2.3.3.2.301.2.199.3 9 5.5c6 3.667 11.4 6.733 16.2 9.2 4.867 2.533 16.867 8.533 36 18 19.2 9.533 42.3 18.967 69.3 28.3s47.334 15.667 61 19c13.667 3.333 32.5 6.667 56.5 10s37.834 5.167 41.5 5.5c3.667.333 8.334.667 14 1l8.5.5v.5H0v-50.5z" fill="#fff" stroke="#fff" stroke-width=".5"/></svg>
  </a>

  <a href="mailto:?subject=Check%20out%20Brief%20Survey%20of%20Position%20Estimation%20Systems.&body=Brief%20Survey%20of%20Position%20Estimation%20Systems%2c%20by%20Merry%20Sprout%0a%3cnil%3e%0a%0ahttps%3a%2f%2fwww.merrysprout.com%2fposts%2fstate_estimation%2fposition_estimator_survey%2f%0a" class="twitter" aria-label="share on Email">
    <svg shape-rendering="geometricPrecision" text-rendering="geometricPrecision" image-rendering="optimizeQuality" xmlns="http://www.w3.org/2000/svg" width="50" height="36" viewBox="7.086 7.087 1277.149 924.008"><path fill="none" d="M1138.734 931.095h.283M1139.017 931.095h-.283"/><path d="M1179.439 7.087c57.543 0 104.627 47.083 104.627 104.626v30.331l-145.36 103.833-494.873 340.894L148.96 242.419v688.676h-37.247c-57.543 0-104.627-47.082-104.627-104.625V111.742C7.086 54.198 54.17 7.115 111.713 7.115l532.12 394.525L1179.41 7.115l.029-.028z" fill="#e75a4d"/><linearGradient id="a" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#a)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><path fill="#e7e4d7" d="M148.96 242.419v688.676h989.774V245.877L643.833 586.771z"/><path fill="#b8b7ae" d="M148.96 931.095l494.873-344.324-2.24-1.586L148.96 923.527z"/><path fill="#b7b6ad" d="M1138.734 245.877l.283 685.218-495.184-344.324z"/><path d="M1284.066 142.044l.17 684.51c-2.494 76.082-35.461 103.238-145.219 104.514l-.283-685.219 145.36-103.833-.028.028z" fill="#b2392f"/><linearGradient id="b" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#b)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><linearGradient id="c" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#c)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><linearGradient id="d" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#d)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><linearGradient id="e" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#e)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><linearGradient id="f" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#f)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><linearGradient id="g" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#g)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><linearGradient id="h" gradientUnits="userSpaceOnUse" x1="1959.712" y1="737.107" x2="26066.213" y2="737.107" gradientTransform="matrix(.0283 0 0 -.0283 248.36 225.244)"><stop offset="0" stop-color="#f8f6ef"/><stop offset="1" stop-color="#e7e4d6"/></linearGradient><path fill="url(#h)" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/><path fill="#f7f5ed" d="M111.713 7.087l532.12 394.525L1179.439 7.087z"/></svg>
  </a>
</div>
    </div>
</div>
<hr>
<div class="row">
    <div class="col-md-12 col-sm-6">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "merrysprout" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
</div>




        </div>  
        <footer>
    <p><center>&copy; 2020 www.merrysprout.com </center></p>
</footer>    

         
<link rel="stylesheet" href="/css/bootstrap_united.min.css">

<script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>    

        <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

        <link rel="stylesheet" type="text/css" href="/css/style.css"  >
        <script src="/js/custom.js"></script>
        
    

    </body>


</html> 